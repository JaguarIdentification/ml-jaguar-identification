{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNR2Q1eqI1DOvv+7xJilsiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidePanza/ml-jaguar-identification/blob/main/notebooks/02_dataset_creation/dino2_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyWktUCyH3eM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "!pip install fiftyone -q\n",
        "import fiftyone as fo\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# import from dino2_utils\n",
        "notebook_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "src_path = os.path.join(notebook_dir, 'src')\n",
        "sys.path.append(src_path)\n",
        "from dino2_utils import DINOv2ArcFace, pad_to_square, setup_transform, get_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "This pipeline is designed to extract embeddings from a set of images stored in a FiftyOne dataset using the DINOv2 model. The embeddings are computed for each image, then saved back to the dataset. These embeddings can be used for further analysis such as clustering, similarity-based retrieval, or training other models for downstream tasks.\n",
        "\n",
        "This process involves:\n",
        "\n",
        "1. Loading the FiftyOne dataset containing images.\n",
        "\n",
        "2. Filtering the dataset to obtain a subset for processing.\n",
        "\n",
        "3. Setting up the DINOv2 model and preparing it for embedding extraction.\n",
        "\n",
        "4. Transforming images using a custom preprocessing pipeline.\n",
        "\n",
        "5. Extracting embeddings from the images and saving them to the dataset.\n",
        "\n",
        "6. Exporting the dataset with embeddings for future use."
      ],
      "metadata": {
        "id": "r-1TbBNN-HFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "The embeddings will be extracted from the images in the uploaded FiftyOne (FO) dataset.\n"
      ],
      "metadata": {
        "id": "Fx3zUMHyJpsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "image_dir = Path('path/to/your/images')\n",
        "input_dir = Path('path/to/your/fo_dataset')\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=str(input_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    rel_dir=image_dir,\n",
        ")\n",
        "\n",
        "# Get train/test set of known jaguars\n",
        "filtered_dataset = dataset.match({\n",
        "    \"testtrainsplit_cosine_similarity\": {\"$in\": [\"train\", \"test\"]}\n",
        "})"
      ],
      "metadata": {
        "id": "H8omdLtIJt-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialise the Model\n",
        "Setup CUDA and load the DINOv2ArcFace model.\n",
        "If a pretrained model is available and `use_pretrained_model` is set to True, load its weights from the given path."
      ],
      "metadata": {
        "id": "SeOQB5ym0bvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the DINOv2ArcFace model in \"embeddings\" mode\n",
        "model = DINOv2ArcFace(usage=\"embeddings\").to(device)\n",
        "\n",
        "# Load pretrained model weights if specified\n",
        "use_pretrained_model = False\n",
        "if use_pretrained_model:\n",
        "    model_path = \"path/to/your/model.pth\"\n",
        "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict)\n"
      ],
      "metadata": {
        "id": "njgJgw3pz5qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Embeddings\n",
        "Extract Embeddings from the filtered dataset using the pretrained DINOv2 model.  \n",
        "* Each image is transformed, passed through the model, and the resulting embedding is saved back to the dataset.  \n",
        "* CLS token from the last hidden state, projected to a lower-dimensional space (512 dim) and normalised is saved back to the dataset.\n"
      ],
      "metadata": {
        "id": "_NdxsRr10eQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise\n",
        "transform = setup_transform(use_padding=True)\n",
        "model.eval()\n",
        "\n",
        "# get embeddings\n",
        "for idx,sample in enumerate(filtered_dataset):\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"Processing sample {idx}/{len(filtered_dataset)}\")\n",
        "    embedding = get_embedding(sample.filepath, model, transform)\n",
        "    # Add the embedding to the sample in a predefined field (e.g., \"dinov2_embedding\")\n",
        "    sample[\"dinov2_embedding\"] = embedding\n", 
        "    sample.save()"
      ],
      "metadata": {
        "id": "yDxtQtXYWksJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store Dataset"
      ],
      "metadata": {
        "id": "vCcqE-BjyUFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store dataset metadata\n",
        "storage_dir = Path('path/to/your/fo_dataset')\n",
        "os.makedirs(storage_dir, exist_ok=True)\n",
        "\n",
        "filtered_dataset.export(\n",
        "    # Directory to save the datasets\n",
        "    export_dir=str(storage_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=False,\n",
        "    rel_dir=image_dir\n",
        ")"
      ],
      "metadata": {
        "id": "iiWUd6z3TVVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
